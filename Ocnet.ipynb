{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af1bf1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4fcccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import config   ### just need to import config for config.py - \n",
    "from checkpoints import CheckpointIO\n",
    "from onet import generation\n",
    "from training import training\n",
    "import models\n",
    "import yaml\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a99e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f194333",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"ocnet.yaml\"\n",
    "with open(path, 'r') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Shorthands\n",
    "out_dir = \"out/onet\"\n",
    "batch_size = 64\n",
    "backup_every = 100000\n",
    "#exit_after = args.exit_after  ### can check later if we really need the exit after\n",
    "\n",
    "model_selection_metric = \"iou\"\n",
    "model_selection_sign = 1\n",
    "\n",
    "# Output directory\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3932d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_remove_none(batch):\n",
    "    ''' Collater that puts each data field into a tensor with outer dimension\n",
    "        batch size.\n",
    "\n",
    "    Args:\n",
    "        batch: batch\n",
    "    '''\n",
    "\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return data.dataloader.default_collate(batch)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    ''' Worker init function to ensure true randomness.\n",
    "    '''\n",
    "    random_data = os.urandom(4)\n",
    "    base_seed = int.from_bytes(random_data, byteorder=\"big\")\n",
    "    np.random.seed(base_seed + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71291e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = SummaryWriter(os.path.join(out_dir, 'logs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "797ba25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = config.get_dataset('train', cfg)\n",
    "val_dataset = config.get_dataset('val', cfg)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=4, shuffle=True,\n",
    "    collate_fn=collate_remove_none,\n",
    "    worker_init_fn=worker_init_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=10, num_workers=4, shuffle=False,\n",
    "    collate_fn=collate_remove_none,\n",
    "    worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ab8ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = config.get_model(cfg, device=device, dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74fa3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize training\n",
    "npoints = 1000\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "trainer = training.Trainer(                         ## set the trainig for training\n",
    "    model, optimizer,\n",
    "    device=device, input_type=\"img\",\n",
    "    vis_dir=\"out/img/onet/vis\", threshold=0.2,\n",
    "    eval_sample=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e3de7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best validation metric (iou): -inf\n"
     ]
    }
   ],
   "source": [
    "## all here to load check point and get info from the check point\n",
    "checkpoint_io = CheckpointIO(out_dir, model=model, optimizer=optimizer)  ##save check poins\n",
    "try:                                                            ##here to load checkpoint to cotinue to train i guess\n",
    "    load_dict = checkpoint_io.load('model.pt')         ## so here model saved as .pt not .ckpt\n",
    "    #load_dict = checkpoint_io.load('model.ckpt')   #so load_dict is a dict for the ckpt file\n",
    "except FileExistsError:\n",
    "    load_dict = dict()\n",
    "epoch_it = load_dict.get('epoch_it', -1)\n",
    "it = load_dict.get('it', -1)\n",
    "metric_val_best = load_dict.get(\n",
    "    'loss_val_best', -model_selection_sign * np.inf)\n",
    "\n",
    "\n",
    "print('Current best validation metric (%s): %.8f'\n",
    "      % (model_selection_metric, metric_val_best))   ## model_selection_metric here is IoU metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6cf1faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OccupancyNetwork(\n",
      "  (decoder): DecoderCBatchNorm(\n",
      "    (fc_p): Conv1d(3, 256, kernel_size=(1,), stride=(1,))\n",
      "    (block0): CResnetBlockConv1d(\n",
      "      (bn_0): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (bn_1): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (block1): CResnetBlockConv1d(\n",
      "      (bn_0): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (bn_1): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (block2): CResnetBlockConv1d(\n",
      "      (bn_0): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (bn_1): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (block3): CResnetBlockConv1d(\n",
      "      (bn_0): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (bn_1): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (block4): CResnetBlockConv1d(\n",
      "      (bn_0): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (bn_1): CBatchNorm1d(\n",
      "        (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "      (fc_0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (fc_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (actvn): ReLU()\n",
      "    )\n",
      "    (bn): CBatchNorm1d(\n",
      "      (conv_gamma): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (conv_beta): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (fc_out): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (encoder): Resnet18(\n",
      "    (features): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Sequential()\n",
      "    )\n",
      "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 13414465\n"
     ]
    }
   ],
   "source": [
    "# Shorthands\n",
    "print_every = cfg['training']['print_every']\n",
    "checkpoint_every = cfg['training']['checkpoint_every']\n",
    "validate_every = cfg['training']['validate_every']\n",
    "visualize_every = cfg['training']['visualize_every']\n",
    "\n",
    "# Print model                                                   ##can remove later- just need to see the number of paras\n",
    "nparameters = sum(p.numel() for p in model.parameters())\n",
    "print(model)\n",
    "print('Total number of parameters: %d' % nparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "90f3c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] it=020, loss=278.8022\n",
      "[Epoch 04] it=030, loss=288.2823\n",
      "[Epoch 04] it=040, loss=269.2306\n",
      "[Epoch 04] it=050, loss=246.6505\n",
      "[Epoch 04] it=060, loss=306.3763\n",
      "[Epoch 04] it=070, loss=240.6857\n",
      "[Epoch 04] it=080, loss=266.3679\n",
      "[Epoch 04] it=090, loss=255.9061\n",
      "[Epoch 04] it=100, loss=190.1610\n",
      "[Epoch 04] it=110, loss=237.9591\n",
      "[Epoch 04] it=120, loss=198.2983\n",
      "[Epoch 04] it=130, loss=241.2523\n",
      "[Epoch 04] it=140, loss=193.7250\n",
      "[Epoch 04] it=150, loss=202.4193\n",
      "[Epoch 04] it=160, loss=180.4740\n",
      "[Epoch 04] it=170, loss=202.3307\n",
      "[Epoch 04] it=180, loss=224.6634\n",
      "[Epoch 04] it=190, loss=222.3888\n",
      "[Epoch 04] it=200, loss=213.4074\n",
      "[Epoch 04] it=210, loss=208.7204\n",
      "[Epoch 04] it=220, loss=178.0703\n",
      "[Epoch 04] it=230, loss=217.4688\n",
      "[Epoch 04] it=240, loss=198.6153\n",
      "[Epoch 04] it=250, loss=231.1876\n",
      "[Epoch 04] it=260, loss=187.0673\n",
      "[Epoch 04] it=270, loss=177.7810\n",
      "[Epoch 04] it=280, loss=200.2459\n",
      "[Epoch 04] it=290, loss=226.7313\n",
      "[Epoch 04] it=300, loss=197.4184\n",
      "[Epoch 04] it=310, loss=198.7884\n",
      "[Epoch 04] it=320, loss=174.2735\n",
      "[Epoch 04] it=330, loss=201.9705\n",
      "[Epoch 04] it=340, loss=196.8801\n",
      "[Epoch 04] it=350, loss=195.7204\n",
      "[Epoch 04] it=360, loss=188.9576\n",
      "[Epoch 04] it=370, loss=191.1283\n",
      "[Epoch 04] it=380, loss=214.9053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m epoch_it \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      6\u001b[0m     it \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_step(batch)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-T2s4-u2a-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-T2s4-u2a-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-T2s4-u2a-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/machine_learning_for_3d_geometry-T2s4-u2a-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    epoch_it += 1\n",
    "    #scheduler.step()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        it += 1\n",
    "        loss = trainer.train_step(batch)\n",
    "        logger.add_scalar('train/loss', loss, it)\n",
    "\n",
    "        # Print output\n",
    "        if print_every > 0 and (it % print_every) == 0:\n",
    "            print('[Epoch %02d] it=%03d, loss=%.4f'\n",
    "                  % (epoch_it, it, loss))\n",
    "\n",
    "        # Visualize output\n",
    "        if visualize_every > 0 and (it % visualize_every) == 0:\n",
    "            print('Visualizing')\n",
    "            trainer.visualize(data_vis)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (checkpoint_every > 0 and (it % checkpoint_every) == 0):\n",
    "            print('Saving checkpoint')\n",
    "            checkpoint_io.save('model.pt', epoch_it=epoch_it, it=it,\n",
    "                               loss_val_best=metric_val_best)\n",
    "\n",
    "        # Backup if necessary\n",
    "        if (backup_every > 0 and (it % backup_every) == 0):\n",
    "            print('Backup checkpoint')\n",
    "            checkpoint_io.save('model_%d.pt' % it, epoch_it=epoch_it, it=it,\n",
    "                               loss_val_best=metric_val_best)\n",
    "        # Run validation\n",
    "        if validate_every > 0 and (it % validate_every) == 0:\n",
    "            eval_dict = trainer.evaluate(val_loader)\n",
    "            metric_val = eval_dict[model_selection_metric]\n",
    "            print('Validation metric (%s): %.4f'\n",
    "                  % (model_selection_metric, metric_val))\n",
    "\n",
    "            for k, v in eval_dict.items():\n",
    "                logger.add_scalar('val/%s' % k, v, it)\n",
    "\n",
    "            if model_selection_sign * (metric_val - metric_val_best) > 0:\n",
    "                metric_val_best = metric_val\n",
    "                print('New best model (loss %.4f)' % metric_val_best)\n",
    "                checkpoint_io.save('model_best.pt', epoch_it=epoch_it, it=it,\n",
    "                                   loss_val_best=metric_val_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "36f94514",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e4d0734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal(loc: torch.Size([0]), scale: torch.Size([0]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.p0_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75d39efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_folder(folder_path, zip_path):\n",
    "    # Create a zip file object\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Iterate over all the files in the folder\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                # Get the file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Add the file to the zip file\n",
    "                zipf.write(file_path, os.path.relpath(file_path, folder_path))\n",
    "\n",
    "# Specify the folder path and zip file path\n",
    "folder_path = 'occupancy_networks_second_ver'\n",
    "zip_path = 'occupancy_networks_third_ver.zip'\n",
    "\n",
    "# Call the function to zip the folder\n",
    "zip_folder(folder_path, zip_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
